We are running a project that aims to capture the underlying structure of conversations, known as 'assemblies' happening asynchronously over email.
Each assembly has a discrete time period, fixed participant set and runs in rounds where batches of responses are synthesised and shared. 
Every assembly has a definite aim; a common aim is to come to consensus over the content of a technical specification, or a position statement on a certain topic. 
There is a separate email client that extracts all the individual response texts for each round and decomposes them into a comprehensive set of distinct ideas or 'positions'.
Those positions are saved in csv files. 

The flow is as follows: A hard-coded prompt is passed to agent "Whisper". Whisper's task is to write effective prompts for two other agents: Spec and Quant.
Spec's task is to generate a specification that can be passed to another agent, "Dev", who builds executable python code to that specification. 
The outcomes from Dev's code execution are passed to Quant, who analyses them and returns a human-readable report full of actionable insights, trend analysis and other materials that help me, the human, understand the dataset.

You will be provided with the initial prompt for Whisper, the prompts passed to Spec, Dev and Quant, and the outputs generated by each agent. 
Your task is to examine the outputs and audit them for rigour, accuracy, veracity, clarity and readability. 
For code outputs, you will also consider code cleanliness, conformance to industry standards, test coverage and efficiency. 
You should then produce learning materials that can be passed to each agent that they can learn from the next time they are invoked. 
You should also provide updated prompts to pass to each agent to make sure they take your feedback into account.
I anticipate this acting as a kind of quasi-reinforcement learning process for each agent, with you acting as the quality assurance engineer and auditor.

To recap, your task is to produce:

- learning materials for each agent to improve their performance in future invocations.
- refined prompts that will improve each agent's performance.
