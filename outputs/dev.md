To extend the existing script according to the technical specification, we will implement the following enhancements:

1. **Sentiment Evolution**: Track sentiment changes per participant across rounds.
2. **Cluster Stability**: Calculate Jaccard similarity between clusters across rounds.
3. **Participant Influence**: Calculate influence scores and centrality using network analysis.
4. **Advanced Topic Modeling**: Replace LDA with BERTopic for more interpretable topics.
5. **Inter-Round Comparison**: Track topic/cluster size and composition across rounds.

Let's start by implementing these enhancements step-by-step. I'll begin with the sentiment evolution analysis.

### Step 1: Sentiment Evolution

We will modify the script to track sentiment changes per participant across rounds and visualize the results.

```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, jaccard_score
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from textblob import TextBlob
from dotenv import load_dotenv
from collections import defaultdict
import seaborn as sns
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
import networkx as nx
import plotly.express as px

# Load environment variables
load_dotenv()

def extract_position_data(file_path):
    """Load text data and extract relevant columns."""
    df = pd.read_csv(file_path)
    return df

def analyze_sentiment_evolution(df):
    """Analyze sentiment evolution per participant across rounds."""
    df['sentiment_polarity'] = df['position_text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)
    df['sentiment_change'] = df.groupby('participant_id')['sentiment_polarity'].diff().fillna(0)
    return df[['participant_id', 'round', 'sentiment_polarity', 'sentiment_change']]

def plot_sentiment_evolution(df_sentiment):
    """Plot sentiment evolution per participant."""
    plt.figure(figsize=(12, 8))
    sns.lineplot(data=df_sentiment, x='round', y='sentiment_polarity', hue='participant_id', marker='o')
    plt.title('Sentiment Evolution per Participant Across Rounds')
    plt.xlabel('Round')
    plt.ylabel('Sentiment Polarity')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig('outputs/sentiment_evolution.png')
    plt.close()

def run_sentiment_evolution_analysis(file_path):
    """Run sentiment evolution analysis."""
    df = extract_position_data(file_path)
    df_sentiment = analyze_sentiment_evolution(df)
    plot_sentiment_evolution(df_sentiment)
    return df_sentiment

# Example usage
if __name__ == "__main__":
    sentiment_evolution_df = run_sentiment_evolution_analysis(file_path=os.getenv("FILE_PATH"))
    print(sentiment_evolution_df.head())
```

### Explanation:
- **Sentiment Analysis**: The `analyze_sentiment_evolution` function calculates sentiment polarity for each statement and computes the change in sentiment across rounds for each participant.
- **Visualization**: The `plot_sentiment_evolution` function creates a line plot to visualize sentiment changes per participant across rounds.

Let's execute this code to verify its correctness.{"code": "import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom textblob import TextBlob\nimport seaborn as sns\n\n# Mock data for testing\ndata = {\n    \"participant_id\": [\"p1\", \"p1\", \"p2\", \"p2\"],\n    \"round\": [1, 2, 1, 2],\n    \"position_text\": [\n        \"I agree with the proposal.\",\n        \"I still agree, but with reservations.\",\n        \"The proposal needs more work.\",\n        \"I now support the proposal.\"\n    ]\n}\ndf = pd.DataFrame(data)\n\ndef analyze_sentiment_evolution(df):\n    \"\"\"Analyze sentiment evolution per participant across rounds.\"\"\"\n    df['sentiment_polarity'] = df['position_text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n    df['sentiment_change'] = df.groupby('participant_id')['sentiment_polarity'].diff().fillna(0)\n    return df[['participant_id', 'round', 'sentiment_polarity', 'sentiment_change']]\n\ndef plot_sentiment_evolution(df_sentiment):\n    \"\"\"Plot sentiment evolution per participant.\"\"\"\n    plt.figure(figsize=(12, 8))\n    sns.lineplot(data=df_sentiment, x='round', y='sentiment_polarity', hue='participant_id', marker='o')\n    plt.title('Sentiment Evolution per Participant Across Rounds')\n    plt.xlabel('Round')\n    plt.ylabel('Sentiment Polarity')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.savefig('outputs/sentiment_evolution.png')\n    plt.close()\n\ndef run_sentiment_evolution_analysis(df):\n    \"\"\"Run sentiment evolution analysis.\"\"\"\n    df_sentiment = analyze_sentiment_evolution(df)\n    plot_sentiment_evolution(df_sentiment)\n    return df_sentiment\n\n# Run the sentiment evolution analysis\nsentiment_evolution_df = run_sentiment_evolution_analysis(df)\nsentiment_evolution_df"}{"code": "from sklearn.metrics import jaccard_score\n\ndef calculate_cluster_stability(clusters_per_round):\n    \"\"\"Calculate Jaccard similarity between clusters across rounds.\"\"\"\n    n_rounds = len(clusters_per_round)\n    jaccard_similarities = []\n    \n    for i in range(n_rounds - 1):\n        for j in range(i + 1, n_rounds):\n            jaccard_sim = jaccard_score(\n                clusters_per_round[i], \n                clusters_per_round[j], \n                average='macro'\n            )\n            jaccard_similarities.append({\n                'round_1': i + 1,\n                'round_2': j + 1,\n                'jaccard_similarity': jaccard_sim\n            })\n    \n    return pd.DataFrame(jaccard_similarities)\n\ndef plot_cluster_stability(df_stability):\n    \"\"\"Plot cluster stability across rounds.\"\"\"\n    stability_matrix = df_stability.pivot(index='round_1', columns='round_2', values='jaccard_similarity')\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(stability_matrix, annot=True, cmap='coolwarm', cbar_kws={'label': 'Jaccard Similarity'})\n    plt.title('Cluster Stability Across Rounds')\n    plt.xlabel('Round')\n    plt.ylabel('Round')\n    plt.tight_layout()\n    plt.savefig('outputs/cluster_stability.png')\n    plt.close()\n\n# Mock clusters for testing\nclusters_per_round = [\n    np.array([0, 0, 1, 1]),  # Round 1\n    np.array([0, 1, 1, 0])   # Round 2\n]\n\ndf_stability = calculate_cluster_stability(clusters_per_round)\nplot_cluster_stability(df_stability)\n\ndf_stability"}{"code": "def calculate_participant_influence(df, clusters_per_round):\n    \"\"\"Calculate influence score and centrality for each participant.\"\"\"\n    # Influence Score: Number of times their statements are clustered with others\n    influence_scores = defaultdict(int)\n    \n    for clusters in clusters_per_round:\n        for i, cluster in enumerate(clusters):\n            for j in range(i + 1, len(clusters)):\n                if clusters[i] == clusters[j]:\n                    participant_i = df.iloc[i]['participant_id']\n                    participant_j = df.iloc[j]['participant_id']\n                    influence_scores[participant_i] += 1\n                    influence_scores[participant_j] += 1\n    \n    # Centrality: Use network analysis to model conversation as a graph\n    G = nx.Graph()\n    \n    for clusters in clusters_per_round:\n        for i, cluster in enumerate(clusters):\n            for j in range(i + 1, len(clusters)):\n                if clusters[i] == clusters[j]:\n                    participant_i = df.iloc[i]['participant_id']\n                    participant_j = df.iloc[j]['participant_id']\n                    if G.has_edge(participant_i, participant_j):\n                        G[participant_i][participant_j]['weight'] += 1\n                    else:\n                        G.add_edge(participant_i, participant_j, weight=1)\n    \n    centrality = nx.degree_centrality(G)\n    \n    influence_df = pd.DataFrame({\n        'participant_id': list(influence_scores.keys()),\n        'influence_score': list(influence_scores.values()),\n        'centrality': [centrality[p] for p in influence_scores.keys()]\n    })\n    \n    return influence_df\n\ndef plot_participant_influence(G):\n    \"\"\"Plot network graph of participant influence.\"\"\"\n    plt.figure(figsize=(10, 8))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, node_size=2000, node_color='skyblue', font_size=10, width=2)\n    plt.title('Network Graph of Participant Influence')\n    plt.tight_layout()\n    plt.savefig('outputs/participant_influence.png')\n    plt.close()\n\n# Calculate participant influence\ninfluence_df = calculate_participant_influence(df, clusters_per_round)\n\n# Create a graph for visualization\nG = nx.Graph()\nfor clusters in clusters_per_round:\n    for i, cluster in enumerate(clusters):\n        for j in range(i + 1, len(clusters)):\n            if clusters[i] == clusters[j]:\n                participant_i = df.iloc[i]['participant_id']\n                participant_j = df.iloc[j]['participant_id']\n                if G.has_edge(participant_i, participant_j):\n                    G[participant_i][participant_j]['weight'] += 1\n                else:\n                    G.add_edge(participant_i, participant_j, weight=1)\n\nplot_participant_influence(G)\n\ninfluence_df"}It seems I forgot to import `defaultdict` from the `collections` module. Let me fix that and re-run the code.{"code": "from collections import defaultdict\n\ndef calculate_participant_influence(df, clusters_per_round):\n    \"\"\"Calculate influence score and centrality for each participant.\"\"\"\n    # Influence Score: Number of times their statements are clustered with others\n    influence_scores = defaultdict(int)\n    \n    for clusters in clusters_per_round:\n        for i, cluster in enumerate(clusters):\n            for j in range(i + 1, len(clusters)):\n                if clusters[i] == clusters[j]:\n                    participant_i = df.iloc[i]['participant_id']\n                    participant_j = df.iloc[j]['participant_id']\n                    influence_scores[participant_i] += 1\n                    influence_scores[participant_j] += 1\n    \n    # Centrality: Use network analysis to model conversation as a graph\n    G = nx.Graph()\n    \n    for clusters in clusters_per_round:\n        for i, cluster in enumerate(clusters):\n            for j in range(i + 1, len(clusters)):\n                if clusters[i] == clusters[j]:\n                    participant_i = df.iloc[i]['participant_id']\n                    participant_j = df.iloc[j]['participant_id']\n                    if G.has_edge(participant_i, participant_j):\n                        G[participant_i][participant_j]['weight'] += 1\n                    else:\n                        G.add_edge(participant_i, participant_j, weight=1)\n    \n    centrality = nx.degree_centrality(G)\n    \n    influence_df = pd.DataFrame({\n        'participant_id': list(influence_scores.keys()),\n        'influence_score': list(influence_scores.values()),\n        'centrality': [centrality[p] for p in influence_scores.keys()]\n    })\n    \n    return influence_df\n\ndef plot_participant_influence(G):\n    \"\"\"Plot network graph of participant influence.\"\"\"\n    plt.figure(figsize=(10, 8))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, node_size=2000, node_color='skyblue', font_size=10, width=2)\n    plt.title('Network Graph of Participant Influence')\n    plt.tight_layout()\n    plt.savefig('outputs/participant_influence.png')\n    plt.close()\n\n# Calculate participant influence\ninfluence_df = calculate_participant_influence(df, clusters_per_round)\n\n# Create a graph for visualization\nG = nx.Graph()\nfor clusters in clusters_per_round:\n    for i, cluster in enumerate(clusters):\n        for j in range(i + 1, len(clusters)):\n            if clusters[i] == clusters[j]:\n                participant_i = df.iloc[i]['participant_id']\n                participant_j = df.iloc[j]['participant_id']\n                if G.has_edge(participant_i, participant_j):\n                    G[participant_i][participant_j]['weight'] += 1\n                else:\n                    G.add_edge(participant_i, participant_j, weight=1)\n\nplot_participant_influence(G)\n\ninfluence_df"}It seems I forgot to import the `networkx` library. Let me fix that and re-run the code.{"code": "import networkx as nx\n\ndef calculate_participant_influence(df, clusters_per_round):\n    \"\"\"Calculate influence score and centrality for each participant.\"\"\"\n    # Influence Score: Number of times their statements are clustered with others\n    influence_scores = defaultdict(int)\n    \n    for clusters in clusters_per_round:\n        for i, cluster in enumerate(clusters):\n            for j in range(i + 1, len(clusters)):\n                if clusters[i] == clusters[j]:\n                    participant_i = df.iloc[i]['participant_id']\n                    participant_j = df.iloc[j]['participant_id']\n                    influence_scores[participant_i] += 1\n                    influence_scores[participant_j] += 1\n    \n    # Centrality: Use network analysis to model conversation as a graph\n    G = nx.Graph()\n    \n    for clusters in clusters_per_round:\n        for i, cluster in enumerate(clusters):\n            for j in range(i + 1, len(clusters)):\n                if clusters[i] == clusters[j]:\n                    participant_i = df.iloc[i]['participant_id']\n                    participant_j = df.iloc[j]['participant_id']\n                    if G.has_edge(participant_i, participant_j):\n                        G[participant_i][participant_j]['weight'] += 1\n                    else:\n                        G.add_edge(participant_i, participant_j, weight=1)\n    \n    centrality = nx.degree_centrality(G)\n    \n    influence_df = pd.DataFrame({\n        'participant_id': list(influence_scores.keys()),\n        'influence_score': list(influence_scores.values()),\n        'centrality': [centrality[p] for p in influence_scores.keys()]\n    })\n    \n    return influence_df\n\ndef plot_participant_influence(G):\n    \"\"\"Plot network graph of participant influence.\"\"\"\n    plt.figure(figsize=(10, 8))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, node_size=2000, node_color='skyblue', font_size=10, width=2)\n    plt.title('Network Graph of Participant Influence')\n    plt.tight_layout()\n    plt.savefig('outputs/participant_influence.png')\n    plt.close()\n\n# Calculate participant influence\ninfluence_df = calculate_participant_influence(df, clusters_per_round)\n\n# Create a graph for visualization\nG = nx.Graph()\nfor clusters in clusters_per_round:\n    for i, cluster in enumerate(clusters):\n        for j in range(i + 1, len(clusters)):\n            if clusters[i] == clusters[j]:\n                participant_i = df.iloc[i]['participant_id']\n                participant_j = df.iloc[j]['participant_id']\n                if G.has_edge(participant_i, participant_j):\n                    G[participant_i][participant_j]['weight'] += 1\n                else:\n                    G.add_edge(participant_i, participant_j, weight=1)\n\nplot_participant_influence(G)\n\ninfluence_df"}{"code": "from bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\n\ndef perform_advanced_topic_modeling(text_data, rounds):\n    \"\"\"Perform topic modeling using BERTopic.\"\"\"\n    topic_model = BERTopic()\n    topics, probs = topic_model.fit_transform(text_data)\n    \n    # Get topic info\n    topic_info = topic_model.get_topic_info()\n    topic_info['round'] = rounds\n    \n    return topic_info\n\ndef plot_topic_prevalence(topic_info):\n    \"\"\"Plot topic prevalence per round.\"\"\"\n    plt.figure(figsize=(12, 8))\n    sns.barplot(data=topic_info, x='round', y='Count', hue='Topic', dodge=False)\n    plt.title('Topic Prevalence per Round')\n    plt.xlabel('Round')\n    plt.ylabel('Number of Statements')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.savefig('outputs/topic_prevalence.png')\n    plt.close()\n\n# Mock rounds for testing\nrounds = [1, 2, 1, 2]\n\n# Perform advanced topic modeling\ntopic_info = perform_advanced_topic_modeling(df['position_text'].tolist(), rounds)\n\n# Plot topic prevalence\nplot_topic_prevalence(topic_info)\n\ntopic_info"}The execution limit for this session has been reached. If you'd like me to continue with the implementation of **Advanced Topic Modeling** using BERTopic and the **Inter-Round Comparison**, please let me know! We can proceed from where we left off.